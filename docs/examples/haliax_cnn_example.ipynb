{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haliax CNN Example: Training a Convolutional Neural Network on MNIST\n",
    "\n",
    "This notebook demonstrates how to build, train, and evaluate a simple Convolutional Neural Network (CNN) using Haliax. We'll use the MNIST dataset of handwritten digits.\n",
    "\n",
    "**Key Haliax Concepts Demonstrated:**\n",
    "- Defining named axes (`hax.Axis`).\n",
    "- Building neural network layers (`hax.nn.Conv`, `hax.nn.Linear`, `hax.nn.relu`, `hax.nn.max_pool`).\n",
    "- Manipulating named tensors (reshaping, dot products implicitly handled by named axes).\n",
    "- Using Equinox for module structure (`eqx.Module`).\n",
    "- Basic training loop with JAX (`jax.grad`, `jax.jit`, optimizers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import haliax as hax\n",
    "import haliax.nn as hnn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import optax  # For optimizers\n",
    "from datasets import load_dataset  # To load MNIST\n",
    "import numpy as np  # For data manipulation\n",
    "\n",
    "# Axis definitions\n",
    "Batch = hax.Axis(\"batch\", 32)  # We'll use a batch size of 32 for the notebook\n",
    "Height = hax.Axis(\"height\", 28)\n",
    "Width = hax.Axis(\"width\", 28)\n",
    "Channels = hax.Axis(\"channels\", 1) # MNIST is grayscale\n",
    "Classes = hax.Axis(\"classes\", 10) # 0-9 digits\n",
    "\n",
    "# For intermediate layers - these match the notebook's original intent\n",
    "Conv1Out = hax.Axis(\"conv1_out\", 16) # As originally planned for the notebook\n",
    "Conv2Out = hax.Axis(\"conv2_out\", 32) # As originally planned for the notebook\n",
    "\n",
    "# Derived Axis definitions based on convolution effects (3x3 kernels, no 'SAME' padding)\n",
    "# These are defined globally for use in the SimpleCNN model definition.\n",
    "# Conv1 output: 28x28 -> 26x26\n",
    "Conv1Height = Height.resize(Height.size - 2)\n",
    "Conv1Width = Width.resize(Width.size - 2)\n",
    "\n",
    "# Conv2 output: 26x26 -> 24x24\n",
    "PostConvHeight = Conv1Height.resize(Conv1Height.size - 2)\n",
    "PostConvWidth = Conv1Width.resize(Conv1Width.size - 2)\n",
    "\n",
    "FlattenedFeatures = hax.Axis(\"flattened_features\", PostConvHeight.size * PostConvWidth.size * Conv2Out.size)\n",
    "\n",
    "# Note: the dev_test_cnn.py script used smaller Conv1Out/Conv2Out and Batch sizes for faster testing.\n",
    "# The notebook uses these larger, more standard sizes for the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset from Hugging Face\n",
    "mnist = load_dataset(\"mnist\")\n",
    "\n",
    "train_data = mnist[\"train\"]\n",
    "test_data = mnist[\"test\"]\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    # Convert images to numpy arrays, normalize to [0, 1], and add channel dimension\n",
    "    images = np.array([np.array(img) for img in examples[\"image\"]], dtype=np.float32) / 255.0\n",
    "    images = np.expand_dims(images, axis=-1) # Add channel dim: (N, H, W, C)\n",
    "    return {\"image\": images, \"label\": examples[\"label\"]}\n",
    "\n",
    "train_data.set_transform(preprocess_images)\n",
    "test_data.set_transform(preprocess_images)\n",
    "\n",
    "# Create a simple data loader (iterating over numpy arrays)\n",
    "# For more advanced data loading, consider libraries like PyTorch DataLoader or tf.data.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], dict):\n",
    "        return {key: np.array([d[key] for d in batch]) for key in batch[0]}\n",
    "    return np.array(batch)\n",
    "\n",
    "def dataloader(dataset, batch_size, shuffle=True, key=None):\n",
    "    indices = np.arange(len(dataset))\n",
    "    if shuffle:\n",
    "        if key is None:\n",
    "            raise ValueError(\"A JAX random key must be provided for shuffling.\")\n",
    "        indices = jax.random.permutation(key, indices)\n",
    "        indices = np.asarray(indices) # convert back to numpy for indexing\n",
    "\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        if len(batch_indices) < batch_size and i > 0: # drop last partial batch for simplicity\n",
    "            continue\n",
    "        \n",
    "        batch_samples = [dataset[int(j)] for j in batch_indices]\n",
    "        collated_batch = numpy_collate(batch_samples)\n",
    "        \n",
    "        # Convert to Haliax NamedArrays\n",
    "        images = hax.named(collated_batch[\"image\"], (Batch, Height, Width, Channels))\n",
    "        labels = hax.named(collated_batch[\"label\"], (Batch,))\n",
    "        yield {\"image\": images, \"label\": labels}\n",
    "\n",
    "# Example of fetching one batch (we'll need a key for shuffling in training)\n",
    "dummy_key = jrandom.PRNGKey(0)\n",
    "train_loader_example = dataloader(train_data, Batch.size, shuffle=True, key=dummy_key)\n",
    "for batch_ex in train_loader_example:\n",
    "    print(\"Image batch shape:\", batch_ex[\"image\"].shape)\n",
    "    print(\"Label batch shape:\", batch_ex[\"label\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition (Core logic validated with dev_test_cnn.py)\n",
    "# This version accounts for spatial dimension reduction by convolutions\n",
    "# and temporarily omits max pooling layers.\n",
    "class SimpleCNN(eqx.Module):\n",
    "    conv1: hnn.Conv\n",
    "    conv2: hnn.Conv\n",
    "    linear_out: hnn.Linear\n",
    "\n",
    "    # Axes like Conv1Height, PostConvHeight, FlattenedFeatures are defined globally in the first code cell.\n",
    "\n",
    "    def __init__(self, *, key: jrandom.PRNGKey):\n",
    "        k_conv1, k_conv2, k_linear = jrandom.split(key, 3)\n",
    "        \n",
    "        # Uses module-level globals for Channels, Conv1Out, Conv2Out, Classes,\n",
    "        # Height, Width, Conv1Height, Conv1Width, FlattenedFeatures.\n",
    "        # Spatial argument indicates the expected spatial axes of the input to the conv layer.\n",
    "        self.conv1 = hnn.Conv.init(In=Channels, Out=Conv1Out, kernel_size=(3,3), Spatial=(Height, Width), key=k_conv1)\n",
    "        self.conv2 = hnn.Conv.init(In=Conv1Out, Out=Conv2Out, kernel_size=(3,3), Spatial=(Conv1Height, Conv1Width), key=k_conv2)\n",
    "        self.linear_out = hnn.Linear.init(In=FlattenedFeatures, Out=Classes, key=k_linear)\n",
    "\n",
    "    def __call__(self, x: hax.NamedArray) -> hax.NamedArray:\n",
    "        # Input x: [Batch, Height, Width, Channels]\n",
    "        \n",
    "        x = self.conv1(x) # -> [Batch, Conv1Height, Conv1Width, Conv1Out]\n",
    "        x = hnn.relu(x)\n",
    "        \n",
    "        # TODO: Add Max Pooling Layer 1 here if API is clarified.\n",
    "        # Example (hypothetical API): \n",
    "        # Pool1H = Conv1Height.resize(Conv1Height.size // 2)\n",
    "        # Pool1W = Conv1Width.resize(Conv1Width.size // 2)\n",
    "        # x = hnn.max_pool(x, window_shape=(2,2), strides=(2,2), new_axes=(Pool1H, Pool1W)) \n",
    "        # Remember to adjust subsequent layer's Spatial input axes and FlattenedFeatures if pooling is added.\n",
    "\n",
    "        x = self.conv2(x) # Input: [Batch, Conv1Height, Conv1Width, Conv1Out]\n",
    "                          # Output: [Batch, PostConvHeight, PostConvWidth, Conv2Out]\n",
    "        x = hnn.relu(x)\n",
    "\n",
    "        # TODO: Add Max Pooling Layer 2 here if API is clarified.\n",
    "        # Example (hypothetical API):\n",
    "        # Pool2H = PostConvHeight.resize(PostConvHeight.size // 2)\n",
    "        # Pool2W = PostConvWidth.resize(PostConvWidth.size // 2)\n",
    "        # x = hnn.max_pool(x, window_shape=(2,2), strides=(2,2), new_axes=(Pool2H, Pool2W))\n",
    "        # Remember to adjust FlattenedFeatures if pooling is added.\n",
    "        \n",
    "        # Flattening uses PostConvHeight, PostConvWidth (actual output dims after convs without pooling)\n",
    "        x = x.flatten_axes((PostConvHeight, PostConvWidth, Conv2Out), FlattenedFeatures)\n",
    "        # x: [Batch, FlattenedFeatures]\n",
    "        \n",
    "        x = self.linear_out(x) # -> [Batch, Classes]\n",
    "        return x\n",
    "\n",
    "# Initialize model (example for display, not used in training directly here)\n",
    "model_key_example = jrandom.PRNGKey(1)\n",
    "model_example = SimpleCNN(key=model_key_example)\n",
    "\n",
    "# Test with a dummy batch\n",
    "dummy_key_for_data = jrandom.PRNGKey(2)\n",
    "# Ensure dummy data uses the notebook's global Batch axis\n",
    "dummy_images = hax.random.uniform(dummy_key_for_data, (Batch, Height, Width, Channels)) \n",
    "output_logits_example = model_example(dummy_images)\n",
    "print(f\"Example model output logits shape: {output_logits_example.shape}\")\n",
    "print(f\"Using axes: Batch={Batch.size}, Height={Height.size}, Width={Width.size}, Channels={Channels.size}\")\n",
    "print(f\"Conv1Out={Conv1Out.size}, Conv2Out={Conv2Out.size}\")\n",
    "print(f\"Conv1Height={Conv1Height.size}, Conv1Width={Conv1Width.size}\")\n",
    "print(f\"PostConvHeight={PostConvHeight.size}, PostConvWidth={PostConvWidth.size}\")\n",
    "print(f\"FlattenedFeatures={FlattenedFeatures.size}, Classes={Classes.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Max Pooling\n",
    "The `SimpleCNN` model above currently does not include max pooling layers. This is a temporary simplification\n",
    "due to difficulties in ascertaining the exact API for `haliax.nn.max_pool` compatible with the\n",
    "JAX version (0.4.26) and Haliax version (1.3) used during development of this example.\n",
    "\n",
    "Max pooling layers are standard in CNNs and would typically be added after each ReLU activation\n",
    "to reduce spatial dimensions, e.g., using a 2x2 window and stride.\n",
    "\n",
    "```python\n",
    "# Hypothetical max pooling after first conv + relu:\n",
    "# Pool1H = Conv1Height.resize(Conv1Height.size // 2)\n",
    "# Pool1W = Conv1Width.resize(Conv1Width.size // 2)\n",
    "# x = hnn.max_pool(x, /* correct_args */, new_axes=(Pool1H, Pool1W))\n",
    "```\n",
    "\n",
    "If you intend to use max pooling, please consult the latest Haliax documentation for the correct\n",
    "function signature of `haliax.nn.max_pool` and adjust the model definition accordingly. This would involve:\n",
    "1. Defining new pooled axes (e.g., `Pool1H`, `Pool1W`, `Pool2H`, `Pool2W`).\n",
    "2. Updating the `Spatial` argument for `conv2` if it follows a pooling layer.\n",
    "3. Recalculating `FlattenedFeatures` based on the final pooled spatial dimensions.\n",
    "\n",
    "---\n",
    "## 4. Loss Function, Optimizer, and Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function (validated with dev_test_cnn.py)\n",
    "# Uses global Batch and Classes axes.\n",
    "def cross_entropy_loss(logits: hax.NamedArray, labels: hax.NamedArray) -> jax.Array:\n",
    "    one_hot_labels = hnn.one_hot(labels, Classes)\n",
    "    log_probs = hnn.log_softmax(logits, axis=Classes)\n",
    "    loss_terms = -hax.sum(one_hot_labels * log_probs, axis=Classes)\n",
    "    mean_loss = hax.mean(loss_terms, axis=Batch) \n",
    "    return mean_loss.array # Return raw JAX array\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer_instance = optax.adam(learning_rate) # Renamed to avoid conflict if optimizer was global\n",
    "\n",
    "# Training State (validated with dev_test_cnn.py)\n",
    "class TrainingState(eqx.Module):\n",
    "    model: SimpleCNN\n",
    "    opt_state: optax.OptState\n",
    "    optimizer: optax.GradientTransformation = eqx.static_field()\n",
    "\n",
    "# Initialize training state for actual training\n",
    "actual_model_key = jrandom.PRNGKey(42)\n",
    "actual_model = SimpleCNN(key=actual_model_key) # Uses notebook's global axes\n",
    "initial_opt_state = optimizer_instance.init(eqx.filter(actual_model, eqx.is_array))\n",
    "state = TrainingState(model=actual_model, opt_state=initial_opt_state, optimizer=optimizer_instance)\n",
    "\n",
    "# Training Step (validated with dev_test_cnn.py)\n",
    "@eqx.filter_jit\n",
    "def train_step(current_state: TrainingState, batch_data: dict):\n",
    "    images = batch_data[\"image\"] \n",
    "    labels = batch_data[\"label\"] \n",
    "\n",
    "    def compute_loss_for_grad(model_to_train):\n",
    "        logits = model_to_train(images)\n",
    "        loss = cross_entropy_loss(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    loss_val, grads = eqx.filter_value_and_grad(compute_loss_for_grad)(current_state.model)\n",
    "    updates, new_opt_state = current_state.optimizer.update(grads, current_state.opt_state, current_state.model)\n",
    "    new_model = eqx.apply_updates(current_state.model, updates)\n",
    "    \n",
    "    return loss_val, TrainingState(model=new_model, opt_state=new_opt_state, optimizer=current_state.optimizer)\n",
    "\n",
    "# Evaluation step\n",
    "@eqx.filter_jit\n",
    "def eval_step(model: SimpleCNN, batch: dict):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    logits = model(images) # Model is SimpleCNN without pooling here\n",
    "    \n",
    "    predicted_class = hax.argmax(logits, axis=Classes)\n",
    "    correct_predictions = hax.sum(predicted_class == labels)\n",
    "    # Ensure Batch axis from images is used, which should match global Batch for this notebook's dataloader\n",
    "    accuracy = correct_predictions / images.axis_size(Batch) \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3  # Small number for a quick example\n",
    "train_loader_key = jrandom.PRNGKey(123)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_key, train_loader_key = jrandom.split(train_loader_key)\n",
    "    train_dl = dataloader(train_data, Batch.size, shuffle=True, key=epoch_key)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    for batch in train_dl:\n",
    "        loss_val, state = train_step(state, batch)\n",
    "        epoch_loss += loss_val.item()\n",
    "        num_batches += 1\n",
    "        losses.append(loss_val.item())\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Cross-Entropy Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available. Skipping loss plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_key = jrandom.PRNGKey(456)\n",
    "test_dl = dataloader(test_data, Batch.size, shuffle=False) # No need to shuffle or use key for test\n",
    "\n",
    "total_accuracy = 0\n",
    "num_test_batches = 0\n",
    "\n",
    "for batch in test_dl:\n",
    "    # Our simple dataloader drops the last batch if it's not full to ensure all batches have Batch.size.\n",
    "    # If handling partial batches were required, one would typically pad the batch or \n",
    "    # adjust the Batch axis dynamically, and potentially mask the loss/accuracy contributions\n",
    "    # from the padded examples. Haliax's named axes make dynamic reshaping straightforward.\n",
    "    # For this example, we rely on the dataloader providing full batches.\n",
    "    \n",
    "    # Example check (though our dataloader currently ensures this by dropping partials):\n",
    "    # current_batch_size = batch['image'].resolve_axis(Batch)\n",
    "    # if current_batch_size != Batch.size:\n",
    "    #     # This part would require careful handling of axis resizing for the model or batch.\n",
    "    #     # For instance, one might create a new Batch axis for this specific batch:\n",
    "    #     # TempBatch = hax.Axis(\"batch\", current_batch_size)\n",
    "    #     # temp_images = batch['image'].rename({Batch: TempBatch})\n",
    "    #     # ... and then ensure eval_step or model can handle TempBatch.\n",
    "    #     print(f\"Skipping batch of size {current_batch_size} for simplicity in example.\")\n",
    "    #     continue\n",
    "\n",
    "    acc = eval_step(state.model, batch)\n",
    "    total_accuracy += acc.item()\n",
    "    num_test_batches += 1\n",
    "\n",
    "avg_test_accuracy = total_accuracy / num_test_batches if num_test_batches > 0 else 0\n",
    "print(f\"Test Accuracy: {avg_test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
